\documentclass{article}
\usepackage{hwopt}
\usepackage{bm}

%%%%%%%%%%%%%%%%%
%     Title     %
%%%%%%%%%%%%%%%%%
\title{Coursework (4) for \emph{Introductory Lectures on Optimization}}
\author{Your name \\ Your ID}
\date{Nov. 17, 2022}

\newcommand{\RBB}{\mathbb{R}}
\newcommand{\xB}{\bm{x}}
\newcommand{\yB}{\bm{y}}
\newcommand{\gB}{\bm{g}}
\newcommand{\pB}{\bm{p}}
\newcommand{\aB}{\bm{a}}
\newcommand{\bB}{\bm{b}}
\newcommand{\domf}{\textrm{dom\;}f}
\newcommand{\normgen}[1]{\left\| #1 \right\|}
\newcommand{\normone}[1]{\left\| #1 \right\|_1}
\newcommand{\normtwo}[1]{\left\| #1 \right\|_2}
\newcommand{\SM}{\mathcal{S}}
\newcommand{\strongconvextype}[2]{\SM_{#1}^{#2}(\RBB^n)}
\newcommand{\strconvliptype}[4]{\SM_{#1, #2}^{#3,#4}(\RBB^n)}

\begin{document}
\maketitle

\begin{excercise}\label{e1}
Prove the following resuls.
For proximal point method, if $f$ is closed and convex and optimal value $f^*$ is finite and attained at $x_*$. We have
\[
f(x_k) - f^* \leq \frac{ \normtwo{x_0 - x_*}^2 }{ 2 \sum_{i=0}^{k-1} t_i}, \quad\textrm{for } k \geq 1.
\]
\end{excercise}

\begin{PROOF}{e1}
bla.bla... bla bla.. bla.
\end{PROOF}

\begin{excercise}\label{e2}
Derive the the dual problem of hard margin SVM.
\end{excercise}

\begin{SOLUTION}{e2}
	bla.bla... bla bla.. bla.
\end{SOLUTION}

\begin{excercise}\label{e3}
For KL divergence defined on the probability simplex,  prove that the upper bound of 
$\bigtriangleup_{\psi} (x^*, x_{1}) $ is $\log n$, for $x_1 = [\frac{1}{n}, \ldots, \frac{1}{n}]$.
\end{excercise}

\begin{PROOF}{e3}
bla.bla... bla bla.. bla.
\end{PROOF}

\end{document}