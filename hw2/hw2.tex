\documentclass{article}
\usepackage{hwopt}
\usepackage{bm}

%%%%%%%%%%%%%%%%%
%     Title     %
%%%%%%%%%%%%%%%%%
\title{Coursework (2) for \emph{Introductory Lectures on Optimization}}
\author{Your name \\ Your ID}
\date{Oct. 8, 2022}

\raggedbottom
\newcommand{\R}{\mathbb{R}}
\newcommand{\xB}{\bm{x}}
\renewcommand{\grad}{\nabla}
\newcommand{\yB}{\bm{y}}
\newcommand{\SM}{\mathcal{S}}
\newcommand{\strongconvextype}[2]{\SM_{#1}^{#2}(\R^n)}
\newcommand{\strconvliptype}[4]{\SM_{#1, #2}^{#3,#4}(\R^n)}

\begin{document}
\maketitle

\begin{excercise}\label{temp}
    univariate functions are in the set of \(\mathcal{F}^1(\R)\):
    \[
        \begin{aligned}
            f(x) &= e^x\\
            f(x) &= \abs{x}^p, p>1\\
            f(x) &= \frac{x^2}{1+\abs{x}}\\
            f(x) &= \abs{x} - \ln(1 + \abs{x})
        \end{aligned}
    \]
\end{excercise}

\begin{PROOF}{temp}
    
\end{PROOF}

\begin{excercise}\label{e1}
    Prove that
    \begin{equation}  
        0 \leq f(\yB) - f(\xB) - \innerprod{\grad f(\xB)}{\yB - \xB} \leq \frac{L}{2} \norm{\xB - \yB}^2\nonumber
    \end{equation}
    holds if we have
    \begin{equation} 
        0 \leq \alpha f(\xB) + (1-\alpha) f(\yB) - f( \alpha \xB + (1-\alpha) \yB) \leq 
            + \alpha (1-\alpha) \frac{L}{2} \norm{\xB - \yB}^2. \nonumber
    \end{equation}
\end{excercise}

\begin{PROOF}{e1}
    Since we have
    \begin{equation}\label{eq:e1_convex}
        \begin{aligned}
            &0 \leq \alpha f(\xB) + (1-\alpha) f(\yB) - f( \alpha \xB + (1-\alpha) \yB),\\
            \iff &f( \alpha \xB + (1-\alpha) \yB) \le \alpha f(\xB) + (1-\alpha) f(\yB).
        \end{aligned}
    \end{equation}    
    According to Eq.~\ref{eq:e1_convex}, we can obtain that \(f(\cdot)\) is a convex function. Based on the properties of convex function, we can get
    \begin{equation}\label{eq:e1_left_le}
        \begin{aligned}
            &f(\yB) \ge f(\xB) + \innerprod{\grad f(\xB)}{\yB - \xB}\\
            \iff & 0 \le f(\yB) - f(\xB) - \innerprod{\grad f(\xB)}{\yB - \xB}
        \end{aligned}
    \end{equation}

    Since we have
    \begin{equation}\label{eq:e1_part2}
        \begin{aligned}
            &\alpha f(\xB) + (1-\alpha) f(\yB) - f( \alpha \xB + (1-\alpha) \yB) \\
            &= \alpha (f(\xB) - f(\yB)) + f(\yB) - f( \alpha \xB + (1-\alpha) \yB)
        \end{aligned}
    \end{equation}
\end{PROOF}

\begin{excercise}\label{e2}
Prove that
\begin{equation} \nonumber 
	f(\xB) + \innerprod{\grad f(\xB)}{\yB - \xB} + \frac{1}{2L} \norm{\grad f(\xB) - \grad f(\yB)}^2 \leq f(\yB)
\end{equation}
holds if we have
\begin{equation} \nonumber 
\alpha f(\xB) + (1-\alpha) f(\yB) \geq f(\alpha \xB + (1-\alpha) \yB) \nonumber 
	+ \frac{\alpha(1-\alpha)}{2L} \norm{\grad f(\xB) - \grad f(\yB)}^2, \nonumber
\end{equation}
\end{excercise}

\begin{PROOF}{e2}
	bla.bla... bla bla.. bla.
\end{PROOF}

\begin{excercise}\label{e3}
Let $f$ be continuously differentiable. Prove that both conditions below, holding for all $\xB, \yB \in \R^n$  and $\alpha \in [0,1]$, are equivalent to inclusion $\strongconvextype{\mu}{1}$
\begin{equation}
	\innerprod{\grad f(\xB) - \grad f(\yB)}{\xB - \yB} \geq \mu \norm{\xB - \yB}^2,
\end{equation}
\begin{align}
	\alpha f(\xB) + (1-\alpha) f(\yB) &\geq f(\alpha \xB + (1-\alpha) \yB )\nonumber\\
	&+ \alpha (1-\alpha) \frac{\mu}{2} \norm{\xB - \yB}^2.
\end{align}
\end{excercise}

\begin{PROOF}{e3}
bla.bla... bla bla.. bla.
\end{PROOF}


\end{document}