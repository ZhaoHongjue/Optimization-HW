\section{Introduction to Ridge Regression}

Suppose that we have input vector \(\xB = {(x_1, x_2, \dots, x_p)}^\top\) and want
to predict a real-valued output \(y\). The linear regression model has the from
\begin{equation}\label{eq:lin-reg}
    f(\xB) = \beta + \sum_{j=1}^p x_j w_j = \innerprod{\wB}{\xB'},
\end{equation}
in which \(\wB = {(\beta, w_1, \dots, w_p)}^\top \in \R^{p+1}\) and 
\(\xB' = {(1, x_1, \dots, x_p)}^\top \in \R^{p+1}\). The linear model either assumes that
the regression function is linear, or that the linear model is reasonable approximation.
Here \(\wB\) is unknown parameter vector.

\begin{figure}[!htbp]
    \centering
    \includegraphics[scale = 0.3]{fig/linear-model.png}
    \caption{Linear model fitting with \(\xB \in \R^2\).}\label{fig:linear-model}
\end{figure}

Based on \emph{Ordinary Least Squares Method} mentioned in Appendix~\ref{app:OLS}, we can obtain
\begin{equation}\label{eq:solution-OLS}
    \hat{\wB}_{\text{OLS}} = {\left(\bm{X}^\top \bm{X}\right)}^{-1} \bm{X}^\top \yB.
\end{equation}

However, according to Eq (\ref{eq:solution-OLS}), \(\hat{\wB}_{\text{OLS}}\) depends on \(\bm{X}^\top\bm{X}\). 
In some cases, \(\bm{X}^\top\bm{X}\) may be \emph{singular} or \emph{nearly singular}. 
When there are many correlated variables in a linear regression model, their coefficients can become poorly determined and
exhibit high variance. In those cases, we call \(\bm{X}\) \emph{ill-conditioned}.
Small changes to elements of \(\bm{X}\) will lead to large changes in \(\bm{X}^\top\bm{X}\). In addition, \(\hat{\wB}_{\text{OLS}}\) may provide a 
good fit to the training data, but it will not fit sufficiently well to the test data.

In order to alleviate this problem, we introduce \emph{ridge regression}. Ridge regression shrinks the regression coefficients by imposing a penalty 
on their size. The ridge coefficients minimize a penalized residual sum of squares,
\begin{equation}\label{eq:beta-rigde}
    \hat{\wB}_{\text{ridge}} = \mathop{\arg\min}_{\wB} \left\{
        \sum_{i=1}^N {\left(y_i - \beta - \sum_{j=1}^p x_{ij} w_j \right)}^2 + \lambda \sum_{j=1}^p w_j^2
    \right\}. 
\end{equation}
Here \(\lambda \ge 0\) is a \emph{complexity parameter} that controls the amount of shrinkage.

Rewriting the criterion in Eq (~\ref{eq:beta-rigde}) in matrix form, we can obtain
\begin{equation}
    \RSS = {(\yB - \bm{Xw})}^\top (\yB - \bm{Xw}) + \lambda \wB^\top \wB,
\end{equation}
and
\begin{equation}\label{eq:ridge-derivative}
    \begin{aligned}
        \pdv{\RSS}{\wB} &= -2\bm{X}^\top (\yB - \bm{Xw}) + 2\lambda\wB\\
        \pdv{\RSS}{\wB}{\wB^\top} &= 2\bm{X}^\top\bm{X} + 2\lambda \bm{I}.
    \end{aligned}
\end{equation}
Let the first derivative in Eq (\ref{eq:ridge-derivative}) be zero, then the ridge regression solution can be expressed as 
\begin{equation}
    \hat{\wB}_{\text{ridge}} = {\left( \bm{X}^T \bm{X} + \lambda \bm{I} \right)}^{-1} \bm{X}^\top \bm{y},
\end{equation}
where \(\bm{I}\) is the \((p+1) \times (p+1)\) \emph{identity matrix}.
The solution adds a positive constant to the diagonal of \(\bm{X}^\top \bm{X}\) before inversion, which makes this problem
nonsigular, even if \(\bm{X}^\top \bm{X}\) is not of full rank.

\begin{figure}[!htbp]
    \centering
    \includegraphics[scale = 0.4]{fig/ridge-reg.png}
    \caption{Estimation picture for ridge regression.}\label{fig:ridge-reg}
\end{figure}